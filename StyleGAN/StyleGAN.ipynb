{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN\n",
    "\n",
    "Generative adversarial networks (GANs) have been around since [2014](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), and have shown steady progress in developing models that can generate realistic images. GANs work by creating two competing networks - a Generator and a Discriminator. The Generator maps a latent vector to an image. The Discriminator evaluates the quality of the generated image relative to some existing dataset. Over the training process, the generated images converge from random noise to images that resemble the target dataset.\n",
    "\n",
    "GANs ideally learn to generate images that *look like* the target dataset without actually *copying* the target dataset. GANs learn some probability distribution over the target dataset and generate images that look like the *could have* come from that dataset, without actually memorizing and regurgitating images from the dataset. In a perfect world, each different latent vector maps to a different output, allowing for infinitely many generated images. Latent inputs that are close in vector space generate images that are similar but not exactly the same. In practice however, GANs can suffer from what is known as Mode Collapse, where the Generator converges to only generating a small, finite set of outputs regardless of the input vector.\n",
    "\n",
    "GANs also suffer from stability problems and are notoriously hard to train. Historically, generating larger images while maintaining the quality and diversity possible at lower scales has proven challenging. Various techniques like [Progressive Growing](https://arxiv.org/abs/1710.10196), careful [weight clipping](https://arxiv.org/abs/1701.07875), [regularization strategies](https://arxiv.org/abs/1704.00028) and [normalization approaches](https://arxiv.org/abs/1802.05957) have been developed to improve the stability of training GANs.\n",
    "\n",
    "Another outstanding mystery in GAN-world is understanding how the latent vector maps to the output. Specifically, what parts of the latent input control aspects of the latent output. How can we break apart the latent vector and disentangle the latent representation to have finer control over the resulting image?\n",
    "\n",
    "In 2018, Nvidia released [StyleGAN](https://arxiv.org/pdf/1812.04948.pdf), a new type of GAN architecture that builds on the history of GAN research to create a fundamentally different kind of generator that tackles the problem of understanding the properties of the latent vector.\n",
    "\n",
    "Instead of starting with a latent vector, StyleGAN first sends the latent vector through a mapping network consisting of several fully connected layers. This allows the generator to learn an intermediate latent space that is best for generating images. The intermediate latent vector is then injected into the generator at different layers, rather than just being a starting template.\n",
    "\n",
    "![](media/stylegan.png)\n",
    "\n",
    "The key finding of StyleGAN was that adding the intermediate latent vector at different layers of the generator caused different effects on the output image. Injection at early layers of the generator influenced major details of the image, while injection at later layers influenced finer details. The result is that the StyleGAN generator can be used to blend different latent vectors. The exact method by which the intermediate latent vector is added to the generator is discussed later.\n",
    "\n",
    "![](media/stylegan_mixing.jpg)\n",
    "\n",
    "StyleGAN is able to train with good stability to images as large as 1024x1024 by training with [progressive growing](https://arxiv.org/pdf/1710.10196.pdf). We start training on 4x4 images. After some time, we grow to 8x8, then 16x16 and so on. When we grow the model, there's a small issue. We now have a new layer that converts a tensor of activations to a 3 channel RGB image. Since this new layer is untrained, it likely produces poor quality images. To prevent this from messing up training, the new RGB layer is phased in.\n",
    "\n",
    "![](media/rgb.png)\n",
    "\n",
    "So that's a quick overview. Letâ€™s start looking at the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.vision.gan import *\n",
    "from fastai.callbacks import *\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('G:/FFHQ/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(bs, size, path, num_workers, noise_size=512):\n",
    "    return (GANItemList.from_folder(path, noise_sz=noise_size)\n",
    "               .split_none()\n",
    "               .label_from_func(noop)\n",
    "               .transform(tfms=[[crop_pad(size=size, row_pct=(0,1), col_pct=(0,1))], []], size=size, tfm_y=True)\n",
    "               .databunch(bs=bs, num_workers=num_workers)\n",
    "               .normalize(stats = [torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5])], do_x=False, do_y=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I resized the main dataset into individual sets for each image size\n",
    "# This has a noticeable effect on training speed\n",
    "data_4 = get_data(256, 4, path/'images_4', 8)\n",
    "data_8 = get_data(256, 8, path/'images_8', 8)\n",
    "data_16 = get_data(128, 16, path/'images_16', 8)\n",
    "data_32 = get_data(128, 32, path/'images_32', 8)\n",
    "data_64 = get_data(48, 64, path/'images_64', 8)\n",
    "data_128 = get_data(22, 128, path/'images_128', 8)\n",
    "data_256 = get_data(10, 256, path/'images_256', 8)\n",
    "data_512 = get_data(4, 512, path/'images_512', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StyleGAN Fundamentals\n",
    "\n",
    "First we start with the basic building blocks of the StyleGAN. StyleGAN uses Equalized Learning Rates for all layers. This is a technique from the [Progressive GAN](https://arxiv.org/pdf/1710.10196.pdf) paper.\n",
    "\n",
    "Layers are typically initialized to a careful distribution to ensure the initial output of the untrained layer has approximately mean zero and standard deviation one. This is to avoid having activations or gradients vanish or explode early in training. Kaiming initialization scales all values by a factor of $c = \\frac{\\sqrt2}{W}$ where $W$ is the length of the weight matrix along one dimension.\n",
    "\n",
    "Equalized learning rates applies the Kaiming constant dynamically during runtime, rather than just at the initialization. The reason for doing this is, to quote the paper, \"somewhat subtle\". Many optimization algorithms normalize gradients in such a way that the scale of the gradient update independent of the scale of the parameter being updated. This leads to some small parameters receiving updates that are too large, and large parameters receiving updates that are too small. The effect is that the learning rate is too high for some parameters and too low for others, leading to instability. The Equalized Learning Weights technique applies Kaiming scaling on every forward pass to ensure all the weights in the model are of a similar range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualLR:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def compute_weight(self, module):\n",
    "        weight = getattr(module, self.name + '_orig')\n",
    "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "        return weight * math.sqrt(2 / fan_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        fn = EqualLR(name)\n",
    "\n",
    "        weight = getattr(module, name)\n",
    "        del module._parameters[name]\n",
    "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "        module.register_forward_pre_hook(fn)\n",
    "\n",
    "        return fn\n",
    "\n",
    "    def __call__(self, module, input):\n",
    "        weight = self.compute_weight(module)\n",
    "        setattr(module, self.name, weight)\n",
    "\n",
    "\n",
    "def equal_lr(module, name='weight'):\n",
    "    EqualLR.apply(module, name)\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualConv2d(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        conv = nn.Conv2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "        self.conv = equal_lr(conv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        linear = nn.Linear(in_dim, out_dim)\n",
    "        linear.weight.data.normal_()\n",
    "        linear.bias.data.zero_()\n",
    "\n",
    "        self.linear = equal_lr(linear)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard convolutional block for the model. It consists of two convolutional layers with leaky relu activations. This sort of convolutional block will be used in the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        kernel_size,\n",
    "        padding,\n",
    "        kernel_size2=None,\n",
    "        padding2=None,\n",
    "        pixel_norm=True,\n",
    "        spectral_norm=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        pad1 = padding\n",
    "        pad2 = padding\n",
    "        if padding2 is not None:\n",
    "            pad2 = padding2\n",
    "\n",
    "        kernel1 = kernel_size\n",
    "        kernel2 = kernel_size\n",
    "        if kernel_size2 is not None:\n",
    "            kernel2 = kernel_size2\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            EqualConv2d(in_channel, out_channel, kernel1, padding=pad1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualConv2d(out_channel, out_channel, kernel2, padding=pad2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv(input)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
